# -*- coding: utf-8 -*-
"""Merqury

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Ymo1RWku9lBeJNTpfuIcBMl29X3_FsX

<a href="https://colab.research.google.com/github/roman-bagdasarian/Classifying-Quantum-Phases-of-Matter/blob/main/FLIQ_Challenge_ClassiqDuQIS_5qub.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Make Sure You Are Ready to Go

$\renewcommand{\ket}[1]{\left| #1 \right\rangle}
\renewcommand{\bra}[1]{\left\langle #1 \right|}
\renewcommand{\braket}[2]{\left\langle #1 | #2 \right\rangle}
\newcommand{\ketbra}[2]{\left| #1 \right\rangle\!\left\langle #2 \right|}$

If you haven't done it yet, try running the following lines of code and use the [registration and installation](https://docs.classiq.io/latest/classiq_101/registration_installations/) page if you are having difficulty setting up your environment.\
Uncomment and run the following command to install or update to the latest version of the Classiq SDK (if not installed yet):
"""

pip install -U classiq

import classiq

"""Uncomment and run the following command if your machine has not been

authenticated yet, you only need to run it once!
"""

pip install keyrings.alt

classiq.authenticate()

"""Now you are good to go!

# Rydberg Phase Diagram

Before starting to code, let us reiterating some theory on Rydberg atoms - the subject of this challenge. They interact via the following Hamiltonian:

$$
H = \frac{\Omega}{2} \sum_{i=1}^N X_i
    - \delta \sum_{i=1}^N n_i
    + \sum_{i \lt j} \frac{\Omega R_b^6 }{(a|i-j|)^6} n_i n_j.
$$

You can find the phase diagram for a $51$-atom chain below. It is obtained by fixing $a=1$ and $\Omega=1$ and varying $\delta$ and $R_b$.

<img src="https://github.com/dmitriikhitrin/Classiq-x-DuQIS-FLIQ-Challenge/blob/main/phase_diagram.png?raw=1" alt="Phase Diagram" width="800">

Fig.1: Phase diagram of the 1D Rydberg Hamiltonian, traced out by (left) bipartite entanglement entropy and (right) expectation value of the number of Rydberg excitations. Plots are obtained using tensor-network representation of the ground states of $H$.

In this challenge, we focus on distinguishing between the $Z2$ phase, where the ground state of $H$ has large overlap with the state $\ket{rgr\ldots gr}$, and the $Z3$ phase, where the ground state overlaps strongly with basis states of the form $\ket{\ldots rggrgg\ldots}$.

Evidently, such systems can be efficiently studied using tensor networks. However, this challenge prepares us for a more realistic scenario in which we only have access to measurement outcomes from the ground state of some Hamiltonian, and our goal is to determine which phase of matter the state belongs to.

# Loading and Processing Measurement Data

*   List item
*   List item



Training data for your model contains measurement results in randomized bases performed on a 51-qubit Rydberg atoms chain. We load training data from the .npz file in the next cell.
"""

!wget https://github.com/dmitriikhitrin/Classiq-x-DuQIS-FLIQ-Challenge/raw/main/training_data.npz

import numpy as np
# You might need to make additional imports depending on your implementation

loaded = np.load("training_data.npz", allow_pickle=True)


unprocessed_features = loaded["features"].tolist()
unprocessed_labels = loaded["labels"].tolist()

print(f'There are {len(unprocessed_features)} data points')
print(f'There were T = {len(unprocessed_features[0])} measurements performed for each data point')
print(f'The measurements were performed on {len(unprocessed_features[0][0])} qubits')
print(f'Example: 2nd experiment result of 8th data point -> {unprocessed_features[7][1]}')
print(f'Example: label for the 8th data point -> {unprocessed_labels[7]}')

"""In the above,
- $\ket{g}$ is the atomic ground state, which is a $+1$-eigenstate of Pauli $Z$
- $\ket{r}$ is the highly excited Rydberg state, which is a $-1$-eigenstate of Pauli $Z$
- $\ket{+} = \frac{1}{\sqrt2}(\ket{g} + \ket{r})$, a $+1$-eigenstate of Pauli $X$
- $\ket{-} = \frac{1}{\sqrt2}(\ket{g} - \ket{r})$, a $-1$-eigenstate of Pauli $X$
- $\ket{+i} = \frac{1}{\sqrt2}(\ket{g} +i\ket{r})$, a $+1$-eigenstate of Pauli $Y$
- $\ket{-i} = \frac{1}{\sqrt2}(\ket{g} -i \ket{r})$, a $-1$-eigenstate of Pauli $Y$.

It is up to you how to convert the features into classical shadows and labels into numbers and then both into training data for your model. For example, you could assign $-1$ to $Z2$ and $+1$ to $Z3$.

**Note:** If you decide to define any helper classes/functions in a separate Python file, please submit it alongside your solution notebook, so we can run and grade it properly
"""

import numpy as np
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split

data       = np.load("training_data.npz", allow_pickle=True)
raw_feats  = data["features"].tolist()
raw_labels = data["labels"].tolist()

N, T, Q0 = len(raw_feats), len(raw_feats[0]), len(raw_feats[0][0])

I2 = np.eye(2, dtype=complex)
basis = {
  'g':  np.array([1,0],complex),
  'r':  np.array([0,1],complex),
  '+':  (1/np.sqrt(2))*np.array([1,1],complex),
  '-':  (1/np.sqrt(2))*np.array([1,-1],complex),
  '+i': (1/np.sqrt(2))*np.array([1,1j],complex),
  '-i': (1/np.sqrt(2))*np.array([1,-1j],complex),
}
σ_op = {k: 3*np.outer(v, v.conj()) - I2 for k, v in basis.items()}
norm = lambda s: '+i' if s.strip().replace('−','-')=='i' else s.strip().replace('−','-')


S = np.zeros((N, Q0, 2, 2), complex)
for i, shots in enumerate(raw_feats):
    acc = np.zeros((Q0,2,2), complex)
    for shot in shots:
        for q, o in enumerate(shot):
            acc[q] += σ_op[norm(o)]
    S[i] = acc / T


σx = np.array([[0,1],[1,0]], complex)
σz = np.array([[1,0],[0,-1]], complex)
ρ  = (S + I2)/3
ex = np.real(np.trace(ρ @ σx, axis1=2, axis2=3))
ez = np.real(np.trace(ρ @ σz, axis1=2, axis2=3))
θx = (np.clip(ex, -1, 1) + 1) * (np.pi/2)
θz = (np.clip(ez, -1, 1) + 1) * (np.pi/2)


Xc    = (θx + 1j*θz).reshape(N, -1)
Xreal = np.hstack([Xc.real, Xc.imag])


Xp    = PCA(n_components=4, random_state=0).fit_transform(Xreal)
mn, mx = Xp.min(0), Xp.max(0)
Xp_norm = (Xp - mn) / np.where(mx-mn == 0, 1, (mx-mn)) * np.pi

X_sub = Xp_norm
y     = np.array([-1 if lbl=='Z2' else +1 for lbl in raw_labels])

print("X_sub:", X_sub.shape, " y:", y.shape)


X_temp, X_test, y_temp, y_test = train_test_split(
    X_sub, y,
    test_size=3,
    random_state=42,
    stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size=3,
    random_state=42,
    stratify=y_temp
)

print(f"Train size: {len(X_train)}  (14 examples)")
print(f" Val size: {len(X_val)}   (3 examples)")
print(f"Test size: {len(X_test)}  (3 examples)")

"""# Defining a Quantum Model

In this section, you will create a QML model for classifying the quantum phases. This will include 3 stages:
- First, you will need to decide on the data encoding scheme, i.e. loading numerical features you obtained above into the quantum circuit.
- Then, you will need to come up with an ansatz - a parametrized quantum circuit, which will be optimized to perform classification.
- Finally, to readout classical information from the quantum model, you will need to perform some sort of measurement on the resultant quantum state. Perhaps, you could extract an expectation value of some Pauli-string $P \in \{I, X, Y, Z\}^{\otimes N}$, so that $\langle P \rangle < b$ is interpreted as $Z2$ and  $\langle P \rangle > b$ is interpreted as $Z3$ for some decision boundary $b$.

There are several approaches to QML in Classiq, linked below.

You may find the following guides useful:
- QML with Classiq: http://docs.classiq.io/latest/user-guide/read/qml_with_classiq_guide/
- Variational Model Example: https://github.com/Classiq/classiq-library/blob/main/algorithms/qaoa/maxcut/qaoa_max_cut.ipynb
- Hybrid QNN: https://docs.classiq.io/latest/explore/algorithms/qml/hybrid_qnn/hybrid_qnn_for_subset_majority/

Although the 2nd guide describes a hybrid model, **you may not implement a hybrid model**, the guide should only be used as a reference as to how to implement QML.

**Warning**: Training using the Classiq PyTorch integration may take a prohibitive amount of time. Consider this when choosing an approach.
"""

from classiq import *
from classiq.execution import *
import numpy as np
from classiq.synthesis import synthesize, show

from classiq import (
    qfunc, CArray, CReal, QArray, QBit, Output,
    allocate, RY, RZ, RX, CZ,
    create_model, Constraints
)
from classiq.execution import (
    ExecutionPreferences, ClassiqBackendPreferences,
    ClassiqSimulatorBackendNames
)
from classiq.synthesis import synthesize, show


feature_length = 4
num_qubits     = 2
num_weights    = 2


@qfunc
def encoding(f: CArray[CReal, feature_length], wires: QArray) -> None:
    for q in range(num_qubits):
        i0 = 2 * q
        RY(theta=f[i0],     target=wires[q])
        RZ(theta=f[i0 + 1], target=wires[q])


@qfunc
def ansatz(weights: CArray[CReal, num_weights], wires: QArray) -> None:

    for i in range(num_qubits):
        RX(theta=weights[0], target=wires[i])

    for i in range(num_qubits):
        CZ(ctrl=wires[i], target=wires[(i+1) % num_qubits])

    for i in range(num_qubits):
        RZ(theta=weights[1], target=wires[i])


@qfunc
def strong_entanglement(wires: QArray) -> None:
    for i in range(num_qubits):
        for j in range(i + 1, num_qubits):
            CZ(ctrl=wires[i], target=wires[j])

@qfunc
def main(
    feature: CArray[CReal, feature_length],
    weights: CArray[CReal, num_weights],
    result:  Output[QArray[QBit]],
) -> None:
    allocate(num_qubits, result)
    encoding(feature, wires=result)
    strong_entanglement(wires=result)
    ansatz(weights,           wires=result)

"""### Synthesis

Before training, you must synthesize your model into a quantum program. Placeholders for your parameters will be automatically generated.

You may find the following documentation useful: https://docs.classiq.io/latest/sdk-reference/synthesis/
"""

NUM_SHOTS = 1000
BACKENDS  = ClassiqBackendPreferences(
    backend_name=ClassiqSimulatorBackendNames.SIMULATOR
)

QMOD  = create_model(
    main,
    execution_preferences=ExecutionPreferences(
        num_shots=NUM_SHOTS,
        backend_preferences=BACKENDS
    ),
    constraints=Constraints(optimization_parameter="no_opt")
)
QPROG = synthesize(QMOD)
show(QPROG)

"""# Training the Model

Here, you will optimize the weights in ansatz, so that the model can distiguish between the phases.

You can find the following Classiq tutorial and documentation useful:
- Execution: https://docs.classiq.io/latest/sdk-reference/execution/
- Execution Session: https://docs.classiq.io/latest/user-guide/execution/ExecutionSession/
- Executing With Parameters: https://docs.classiq.io/latest/qmod-reference/language-reference/quantum-entry-point/

It is highly recommended to use an ExecutionSession if you are executing the same circuit with different parameters many times. It is not needed to train parameters using the Classiq PyTorch integration.

If you are not using the PyTorch integration, you will need an objective (also known as a 'loss', or 'cost') function. Depending on your implementation, you will need to either minimize or maximize it in training.
"""

import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from classiq.execution import ExecutionSession, ExecutionPreferences
from classiq._internals.client import ClassiqAPIError


def make_session(shots: int, fast: bool) -> ExecutionSession:
    prefs = ExecutionPreferences(
        num_shots=shots,
        backend_preferences=BACKENDS,
        fast=fast
    )
    return ExecutionSession(quantum_program=QPROG,
                            execution_preferences=prefs)

sess64   = make_session(256,  fast=True)
sess256  = make_session(512, fast=False)
sess_val = make_session(512, fast=False)

def bind(tag, arr):
    return {f"{tag}_param_{i}": float(v) for i, v in enumerate(arr)}

def avg_z(counts):
    tot = sum(counts.values())
    return sum(
        c * sum(+1 if b=='0' else -1 for b in bits)
        for bits, c in counts.items()
    ) / (tot * num_qubits)

def batch_forward(sess, thetas, Xb):
    plist = [
        {**bind("weights", w), **bind("feature", x)}
        for w, x in zip(thetas, Xb)
    ]
    results = sess.batch_sample(parameters=plist)
    cnts = [
        r.counts if isinstance(r.counts, dict) else r.counts[0]
        for r in results
    ]
    return np.array([avg_z(c) for c in cnts])

def safe_batch_forward(sess, thetas, Xb, max_retries=3, delay=0.2):
    for attempt in range(1, max_retries+1):
        try:
            out = batch_forward(sess, thetas, Xb)
            time.sleep(delay)
            return out
        except ClassiqAPIError as e:
            print(f"⚠️ API error (attempt {attempt}): {e}")
            time.sleep(delay * attempt)
    raise RuntimeError("batch_forward failed after retries")

def hinge(sess, w, X, y):
    N = X.shape[0]
    logits = safe_batch_forward(sess, np.tile(w, (N,1)), X)
    return np.mean(np.maximum(0, 1 - y * logits))

def compute_logits(sess, w, X):
    N = X.shape[0]
    return safe_batch_forward(sess, np.tile(w, (N,1)), X)

train_loss_log = []
val_loss_log   = []
best_acc       = -1.0
best_theta     = None


alpha = 0.50
c0    = 0.40
gamma = 0.02
MAX_IT = 120
theta = 0.01 * np.random.randn(num_weights)


for k in range(MAX_IT):
    sess = sess64 if k < 50 else sess256

    ck    = c0 / ( (k+1)**gamma )
    delta = np.random.choice([+1, -1], num_weights)


    lp   = hinge(sess, theta + ck*delta, X_train, y_train)
    lm   = hinge(sess, theta - ck*delta, X_train, y_train)
    grad = (lp - lm) / (2 * ck * delta)
    theta -= alpha * grad


    if k % 20 == 0 or k == MAX_IT - 1:
        L_train   = hinge(sess, theta, X_train, y_train)
        logits_tr = compute_logits(sess, theta, X_train)
        preds_tr  = np.sign(logits_tr)
        y_tr_bin  = (y_train == +1).astype(int)
        p_tr_bin  = (preds_tr == +1).astype(int)
        acc_train = np.mean(preds_tr == y_train)
        prec      = precision_score(y_tr_bin, p_tr_bin, zero_division=0)
        rec       = recall_score(   y_tr_bin, p_tr_bin, zero_division=0)
        f1s       = f1_score(       y_tr_bin, p_tr_bin, zero_division=0)


        L_val = hinge(sess_val, theta, X_val, y_val)

        epoch = len(train_loss_log) + 1
        print(
            f"Epoch {epoch:02d} | "
            f"train loss={L_train:.4f} | "
            f"val loss={L_val:.4f} | "
            f"train acc={acc_train*100:5.2f}% | "
            f"precision={prec*100:5.2f}% | "
            f"recall={rec*100:5.2f}% | "
            f"F1={f1s*100:5.2f}%"
        )

        train_loss_log.append(L_train)
        val_loss_log.append(L_val)
        if acc_train > best_acc:
            best_acc   = acc_train
            best_theta = theta.copy()

sess64.close()
sess256.close()
sess_val.close()

print(f"\nBest train accuracy seen: {best_acc*100:.2f}%")

"""Training that takes too long may make it impossible to grade your submission."""

import matplotlib.pyplot as plt

epochs = list(range(1, len(train_loss_log) + 1))

plt.figure(figsize=(6,4))
plt.plot(epochs, train_loss_log, '-o', label='Train Loss')
plt.plot(epochs, val_loss_log,   '-s', label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.tight_layout()
plt.show()

"""# Testing the Model

Good job! Now it's time to see whether the model you designed can successfully perform the classification. For this, compare the predictions of your model to the actual labels.

If the model does not perform well, try modifying the encoding and/or the ansatz (by using different number of parameters/qubits/ansatz layers/...)
"""

print("X_test.shape:", X_test.shape)
print("y_test.shape:", y_test.shape)
print("best_theta.shape:", best_theta.shape)

import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from classiq.execution import ExecutionSession


sess_test = make_session(256, fast=False)


N_test = X_test.shape[0]
logits_test = safe_batch_forward(
    sess_test,
    np.tile(best_theta, (N_test, 1)),
    X_test
)


preds_test = np.where(logits_test > 0, +1, -1)


acc_test = accuracy_score(y_test, preds_test)
print(f"Test accuracy: {acc_test*100:.2f}%\n")


cm = confusion_matrix(y_test, preds_test, labels=[-1, +1])
print("Confusion matrix (rows=true, cols=predicted):")
print("         Pred Z2   Pred Z3")
print(f"True Z2    {cm[0,0]:>3d}        {cm[0,1]:>3d}")
print(f"True Z3    {cm[1,0]:>3d}        {cm[1,1]:>3d}\n")

y_test_bin  = (y_test   == +1).astype(int)
preds_bin   = (preds_test == +1).astype(int)
print("Classification report:")
print(classification_report(
    y_test_bin,
    preds_bin,
    target_names=["Z2", "Z3"],
    digits=4
))

sess_test.close()

"""## Grading

You will be evaluated on the accuracy, depth, width of your model and the number of parameters in your model.

The following function will return the width and depth of your model as they will be used in grading. Use it to self-evaluate your model.
"""

from classiq import QuantumProgram

def get_metrics(qprog):
    """
    Extract circuit metrics from a quantum program.

    Parameters:
        qprog: The quantum program object.

    Returns:
        dict: A dictionary containing the circuit metrics:
              - "depth": Circuit depth
              - "width": Circuit width
    """
    circuit = QuantumProgram.from_qprog(qprog)

    metrics = {
        "depth": circuit.transpiled_circuit.depth,
        "width": circuit.data.width,
    }

    return metrics

print(get_metrics(QPROG))

sess_test = make_session(256, fast=False)

logits_test = compute_logits(sess_test, best_theta, X_test)
preds_test  = np.sign(logits_test)

y_test_bin = (y_test == +1).astype(int)
p_test_bin = (preds_test == +1).astype(int)

acc_test   = np.mean(preds_test == y_test)
prec_test  = precision_score(y_test_bin, p_test_bin, zero_division=0)
rec_test   = recall_score(   y_test_bin, p_test_bin, zero_division=0)
f1_test    = f1_score(       y_test_bin, p_test_bin, zero_division=0)

print(f"\n Test Accuracy: {acc_test*100:.2f}%")
print(f"Precision: {prec_test*100:.2f}% | Recall: {rec_test*100:.2f}% | F1-score: {f1_test*100:.2f}%")

sess_test.close()

from sklearn.metrics import accuracy_score


A = accuracy_score(y_test, preds_test)


P = num_weights

from classiq import QuantumProgram
metrics = get_metrics(QPROG)
D = metrics["depth"]
W = metrics["width"]

f_score = A - 0.1 * P - 0.0002 * D - 0.1 * W

print(f"Test accuracy (A):       {A*100:5.2f}%")
print(f"Num parameters (P):      {P}")
print(f"Circuit depth (D):       {D}")
print(f"Circuit width (W):       {W}")
print(f"\nFinal filter score (f):  {f_score:.4f}")

"""# Submission

You will submit this notebook, your trained parameters, and your quantum model.
"""

# Do not change this cell

import os

def save_qprog(qprog, team_name: str, folder="."):
    assert isinstance(team_name, str)
    file_name = f"{team_name.replace(' ','_')}.qprog"
    with open(os.path.join(folder, file_name), 'w') as f:
        f.write(qprog.model_dump_json(indent=4))

def save_params(params, team_name: str, folder="."):
    assert isinstance(team_name, str)
    file_name = f"{team_name.replace(' ','_')}.npz"
    with open(os.path.join(folder, file_name), 'wb') as f:
        np.savez(f, params=params)

# Change to your team name!!
TEAM_NAME = "MerQury"

# Insert your trained parameters here!
TRAINED_PARAMS = theta

save_qprog(QPROG, team_name=TEAM_NAME)
save_params(params=TRAINED_PARAMS, team_name=TEAM_NAME)